{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Open from 22 December, 2024 Welcome to my personal library! \u6b22\u8fce\u6765\u5230\u6211\u7684\u56fe\u4e66\u9986\uff01","title":"Home"},{"location":"index_icons/","text":"Open from 22 December, 2024 Welcome to my personal library! ### Installation MkDocs To install MkDocs, run the following command from the command line: ```bash pip install mkdocs Installation theme Install the theme using PIP: pip install mkdocs-simple-blog Activating theme After the theme is installed, edit your mkdocs.yml file and set the theme name to simple-blog : theme: name: simple-blog Getting Help We use GitHub issues for tracking bugs and feature requests and have limited bandwidth to address them. If you need anything, I ask you to please follow our templates for opening issues or discussions. \ud83d\udc1b Bug Report \ud83d\udcd5 Documentation Issue \ud83d\ude80 Feature Request \ud83d\udcac General Question Commit Style \u2699\ufe0f FEATURE \ud83d\udcdd PEP8 \ud83d\udccc ISSUE \ud83e\udeb2 BUG \ud83d\udcd8 DOCS \ud83d\udce6 PyPI \u2764\ufe0f\ufe0f TEST \u2b06\ufe0f CI/CD \u26a0\ufe0f SECURITY License This project is licensed under the terms of the MIT license.","title":"Index icons"},{"location":"index_icons/#installation-theme","text":"Install the theme using PIP: pip install mkdocs-simple-blog","title":"Installation theme"},{"location":"index_icons/#activating-theme","text":"After the theme is installed, edit your mkdocs.yml file and set the theme name to simple-blog : theme: name: simple-blog","title":"Activating theme"},{"location":"index_icons/#getting-help","text":"We use GitHub issues for tracking bugs and feature requests and have limited bandwidth to address them. If you need anything, I ask you to please follow our templates for opening issues or discussions. \ud83d\udc1b Bug Report \ud83d\udcd5 Documentation Issue \ud83d\ude80 Feature Request \ud83d\udcac General Question","title":"Getting Help"},{"location":"index_icons/#commit-style","text":"\u2699\ufe0f FEATURE \ud83d\udcdd PEP8 \ud83d\udccc ISSUE \ud83e\udeb2 BUG \ud83d\udcd8 DOCS \ud83d\udce6 PyPI \u2764\ufe0f\ufe0f TEST \u2b06\ufe0f CI/CD \u26a0\ufe0f SECURITY","title":"Commit Style"},{"location":"index_icons/#license","text":"This project is licensed under the terms of the MIT license.","title":"License"},{"location":"nav/getting-started/","text":"March 25, 2023 Install Installation MkDocs To install MkDocs, run the following command from the command line: pip install mkdocs Installation theme Install the theme using PIP: pip install mkdocs-simple-blog Activating theme After the theme is installed, edit your mkdocs.yml file and set the theme name to simple-blog : theme: name: simple-blog","title":"Getting started"},{"location":"nav/getting-started/#install","text":"","title":"Install"},{"location":"nav/getting-started/#installation-mkdocs","text":"To install MkDocs, run the following command from the command line: pip install mkdocs","title":"Installation MkDocs"},{"location":"nav/getting-started/#installation-theme","text":"Install the theme using PIP: pip install mkdocs-simple-blog","title":"Installation theme"},{"location":"nav/getting-started/#activating-theme","text":"After the theme is installed, edit your mkdocs.yml file and set the theme name to simple-blog : theme: name: simple-blog","title":"Activating theme"},{"location":"nav/random_thoughts/","text":"\ud83d\udc3e SI projection and FID \ud83c\udf38 What is the difference between SI-Projection and FID? The SI-projection and FID (Free Induction Decay) are two different concepts in Magnetic Resonance Imaging (MRI) and spectroscopy, though they are related in the context of data acquisition and signal interpretation: 1. Free Induction Decay (FID) Definition : FID is the raw signal detected immediately after the RF excitation pulse in MRI or spectroscopy. It represents the decaying sinusoidal signal generated by precessing magnetic moments of nuclei as they return to equilibrium. Characteristics : It is the time-domain signal acquired directly by the receiver coil. Contains all frequency components of the excited spins. The decay is due to T2* relaxation , which encompasses both dephasing caused by spin-spin interactions and magnetic field inhomogeneities. Usage : In spectroscopy, FID is Fourier transformed to extract the frequency-domain spectrum. In imaging, FID can be sampled to generate k-space data, which is later transformed into an image. 2. SI-Projection (Superior-Inferior Projection) Definition : SI-projection refers to a 1D or reduced-dimensionality representation of the MRI signal along the superior-inferior (head-to-foot) anatomical direction. It can be considered as a summation or integration of the MRI signal over the other spatial dimensions. Characteristics : It's a spatially resolved signal that contains positional information along the superior-inferior axis. Typically used in projection-based imaging methods or when simplifying multi-dimensional data. May be used for quick anatomical localization or as an intermediate step in reconstruction workflows. Usage : In motion-resolved imaging, SI-projections are often used to monitor respiratory or cardiac motion. For example, signals binned along the SI-axis can capture diaphragm movement during breathing. Key Differences Aspect FID SI-Projection Nature Time-domain raw signal Spatial projection signal Dimension 1D (time) 1D (spatial) Information Type Frequency and phase Spatial (position-based) Purpose Basis for Fourier transforms to generate spectra or k-space Basis for motion tracking or projection imaging Dependence Purely signal decay (T2*) Aggregates signal spatially Example in Practice FID : Directly acquired after the RF pulse. Used in spectroscopy or as input for advanced reconstruction techniques in imaging. SI-Projection : Derived from imaging data along the superior-inferior axis. Often used in motion detection, like tracking respiratory motion during MRI. \ud83c\udf38 OK, then for the raw data readouts from 3D radial trajectory, how can I extract them respectively? Extracting FID and SI-projection from the raw data acquired using a 3D radial trajectory requires understanding the data's structure and processing steps. Here's how you can approach it: 1. Understanding the Raw Data 3D Radial Trajectory : In a 3D radial acquisition, k-space is sampled along radial spokes that span the 3D volume. Each readout corresponds to a radial trajectory in k-space. Data typically includes both real and imaginary parts of the signal for each sampled point. Key Concepts : FID : The signal along the time dimension for a single trajectory before gridding or Fourier Transform. SI-Projection : Data integrated or projected along the superior-inferior axis after some spatial processing. 2. Extracting FID The FID is essentially the raw time-domain signal acquired during each readout, and it can be extracted as follows: Identify the Readout Dimension : Locate the time-domain dimension in your raw data array (e.g., [readout, spokes, coils] ). Each trajectory in k-space corresponds to an FID signal. How to Extract FID: The FID is the very first sample of the raw readout for each spoke and each coil: fid = raw_data[0, :, coil_index] - `0`: Index for the first point in the readout (central k-space). - `:`: All spokes. - `coil_index`: A specific receive coil. 3. Extracting SI-Projection The SI-projection requires spatial information, so some level of reconstruction or processing is needed: Reconstruct SI Direction : Use the radial trajectory information to identify k-space points corresponding to the superior-inferior axis. If trajectory data is stored in kx , ky , kz arrays, filter points for the SI-axis (e.g., where kx \u2248 ky \u2248 0 ). Sum or Integrate Data : Aggregate the raw data over the orthogonal dimensions: si_projection = raw_data.sum(axis=orthogonal_axes) - For a specific coil: si_projection = np.sum(raw_data[:, si_indices, coil_index], axis=1) Motion Binning (Optional) : If used for motion analysis, apply respiratory or cardiac binning based on temporal information or physiological signals. 4. Practical Notes Trajectory Information : Use the trajectory file (often provided by the scanner) to map k-space points to spatial coordinates. FFT for Image Space : If you need to convert raw k-space data to image space, perform an FFT on the summed or integrated k-space data along the appropriate dimensions. Coordinate Transformations : Ensure proper scaling and centering of k-space before projection or reconstruction. The suitability of their applications and motion characteristics Both FID and SI-projection are valuable tools for detecting motion in MRI, but they differ significantly in how they are generated and what types of motion they can capture. Their suitability depends on the specific application and motion characteristics. 1. FID (Free Induction Decay) Key Features: What It Captures : The FID is sensitive to bulk magnetization changes , such as those caused by large-scale motion, scanner instability, or physiological processes that affect the entire signal simultaneously. Captures variations over time at the center of k-space, reflecting the overall signal behavior. How It Works for Motion Detection : Motion affects the uniformity of the magnetic field, causing variations in the FID amplitude or phase. Large-scale motion (e.g., head movement) can disrupt the signal decay pattern. Strengths : High sensitivity to gross motion : Since it captures the central k-space signal, even small deviations in motion can be detected as phase or amplitude variations. No spatial information required : FID does not depend on spatial encoding, making it computationally lightweight and simple to analyze. Ideal for detecting global motion patterns , such as large head movements. Limitations : Lacks spatial specificity: Cannot localize motion along specific axes or regions. Less effective for subtle, localized motion or small physiological movements (e.g., breathing, eye motion). 2. SI-Projection (Superior-Inferior Projection) Key Features: What It Captures : SI-projection aggregates data along the superior-inferior axis of the body, providing a motion-sensitive signal that captures shifts in this specific spatial direction. Reflects changes in signal distribution along the axis, making it more spatially specific than FID. How It Works for Motion Detection : Tracks changes in the superior-inferior axis signal due to respiratory motion , cardiac motion , or localized shifts (e.g., diaphragm or organ movement). Useful in motion-binned reconstruction methods to sort data into motion states. Strengths : Direction-specific : Captures motion primarily along the SI axis, which is beneficial for tracking respiratory motion or vertical head movements. Allows motion binning : By grouping data based on SI-projection values, you can segment the data into different motion states for retrospective correction. More sensitive to subtle, periodic physiological motion (e.g., respiration or heartbeat). Limitations : Less sensitive to non-SI motion: Does not effectively capture lateral (left-right) or anterior-posterior movements. Requires spatial trajectory information, increasing processing complexity compared to FID. Comparison Table Feature FID SI-Projection Captures Global magnetization changes Localized motion along SI axis Spatial Specificity None (global signal) High (specific to SI-axis motion) Motion Sensitivity Large-scale, global motion Subtle, periodic motion (e.g., respiration) Computation Lightweight, no trajectory needed Requires trajectory, spatial processing Best For Detecting global motion or scanner instabilities Tracking respiration or periodic SI motion 3. Which to Use When? Use FID When : The primary concern is gross motion , such as patient movement or scanner instabilities. Simplicity is crucial (no need for spatial information or projections). You are monitoring the signal for artifacts in real-time (e.g., during acquisition). Use SI-Projection When : The focus is on physiological motion such as respiration or cardiac motion. You need motion-resolved reconstructions , where SI-projections enable binning into different motion states. Spatial specificity along the superior-inferior axis is necessary (e.g., for organ tracking or respiratory gating). Practical Applications Application Preferred Method Reason Head motion detection FID Captures global signal disturbances due to head shifts. Respiratory gating SI-Projection Sensitive to motion along the SI axis (e.g., diaphragm movement). Real-time motion artifact monitoring FID Simple, computationally efficient, no spatial info needed. Cardiac motion tracking SI-Projection Captures subtle periodic motion along the SI axis. Motion binning SI-Projection Enables grouping of data into distinct motion states. 4. Combining Both In some cases, FID and SI-projection can be used together : Use FID to monitor large-scale, abrupt motion (e.g., head movement). Use SI-projection for subtle, repetitive motion like respiration or cardiac cycles. By combining these approaches, you can achieve robust motion detection across a wide range of conditions. Let me know if you'd like help implementing this in your dataset! \ud83c\udf38 How can SI-projection be used in eye motion detection? Using SI-projection to monitor eye motion can be effective, but its applicability depends on the specific characteristics of the eye motion being studied and how well the motion aligns with the superior-inferior (SI) axis. Here's a breakdown: 1. Eye Motion Characteristics and SI-Projections Types of Eye Motion : Fixations : Minimal motion as the gaze focuses on a single point. Saccades : Rapid shifts in gaze between fixation points, often multidirectional. Microsaccades : Small, involuntary eye movements during fixation. Smooth Pursuit : Continuous tracking of a moving target. Motion Along the SI-Axis : If the primary eye motion occurs along the SI-axis (e.g., vertical saccades), the SI-projection would be sensitive to these changes. For horizontal (left-right) or oblique movements, the SI-projection would capture only a fraction of the overall motion, limiting its utility. 2. Advantages of Using SI-Projection for Eye Motion Sensitivity to Vertical Motion : Eye movements with a strong vertical component (e.g., vertical saccades, gaze shifts, or blinking) will produce clear variations in the SI-projection. Can capture eyelid motion during blinks, as this involves a significant SI-axis component. Low Computational Complexity : SI-projections aggregate signal along the vertical axis, reducing the dimensionality of data and simplifying analysis. Applications in Eye Motion Studies : Blink Detection : Blinks primarily involve vertical motion of the eyelids, making SI-projection an effective tool. Vertical Gaze Tracking : Changes in vertical eye position can be directly observed in the projection signal. Motion Binning : SI-projections can sort data into states of upward, downward, or stationary gaze for further analysis. 3. Challenges and Limitations Reduced Sensitivity to Horizontal or Oblique Motion : Eye motion along other axes (e.g., left-right saccades) may not produce significant variations in the SI-projection. Combining SI-projection with other projections (e.g., along the left-right or anterior-posterior axis) may be necessary for comprehensive motion tracking. Small Amplitude of Eye Movements : Eye movements, especially microsaccades or smooth pursuit, generate smaller displacements compared to gross motion (e.g., respiration). This requires high sensitivity in the SI-projection data. Artifact Sensitivity : Noise or signal disturbances unrelated to eye motion (e.g., system vibrations) might interfere with SI-projection signals. 4. Practical Considerations To use SI-projection for eye motion detection , you might follow these steps: Step 1: Preprocessing Data Source : Use raw k-space data or reconstructed signals, ensuring the trajectory is well-sampled. Trajectory Alignment : Verify that the SI-axis in the trajectory corresponds to the physical superior-inferior direction of the eye. Step 2: Extract SI-Projection Aggregate k-space data along the SI-axis: si_projection = np.sum(raw_data[:, si_indices, :], axis=1) Step 3: Analyze Motion Look for abrupt changes in SI-projection signals to detect vertical saccades or blinks: diff_signal = np.diff(si_projection, axis=0) Use thresholds to classify motion events (e.g., saccades, blinks, or fixations). Step 4: Validate Against Ground Truth Compare SI-projection results with independent eye-tracking data (e.g., from an EyeLink system) to ensure accuracy. 5. Example Application Here\u2019s how SI-projection could be applied for a blink detection task: Case: Blink Detection What Happens During a Blink? Eyelid movement occurs along the SI-axis, creating a sharp spike in the SI-projection signal. Signal Extraction : Calculate the SI-projection for each time point. Detect peaks or high-amplitude changes that correspond to eyelid motion. Results : The SI-projection provides a time-series representation of eye motion along the SI-axis. Peaks in the signal represent blink events. Comparison to Other Techniques Feature SI-Projection for Eye Motion Dedicated Eye-Tracking Systems (e.g., EyeLink) Sensitivity High for vertical motion or blinks High for all types of eye motion Spatial Resolution Limited to the SI-axis Full 2D/3D tracking of eye position Ease of Use Low computational overhead Requires dedicated hardware and calibration Best Use Case Blinks, vertical gaze shifts Saccades, smooth pursuit, and detailed tracking Conclusion The SI-projection can be a simple, effective tool for detecting vertical eye motion , especially blinks and vertical gaze shifts. However, its application is limited to motion along the SI-axis, making it less effective for horizontal or complex multidirectional movements. For comprehensive eye motion analysis, pairing SI-projections with a dedicated eye-tracking system or additional k-space projections would provide better coverage. Let me know if you'd like help setting up such an analysis pipeline! \ud83c\udf38 Then can I project it alternatively to another direction, say left-right projection instead of SI-projection. Yes, you can project the data along any direction (such as left-right, anterior-posterior, or any oblique direction) instead of the superior-inferior (SI) axis. This involves aggregating the k-space data along the desired direction or axis to extract a corresponding projection. Here's how you can do it step by step: 1. Concept of Alternative Projections Projection Direction : The SI-projection aggregates signal intensity along the superior-inferior axis. Similarly, you can compute a projection along: Left-Right (LR) : Captures horizontal motion or changes along the x-axis. Anterior-Posterior (AP) : Captures depth or forward-backward changes along the y-axis. Custom/Oblique Directions : Projects data onto a diagonal or other arbitrary axis. Utility : Choose the direction depending on the dominant motion in your data or the type of motion you're studying: LR projection is better for horizontal eye movements (e.g., horizontal saccades). AP projection may be relevant for depth changes or slight forward/backward tilting. 2. Implementation Steps Step 1: Identify the Desired Axis Identify the direction in k-space that corresponds to the anatomical axis of interest. Example : SI-axis = Vertical direction = z z LR-axis = Horizontal direction = x x AP-axis = Depth direction = y y Step 2: Aggregate Along the Selected Axis Aggregate the k-space data along the desired axis by summing or integrating the data. # Example with numpy arrays lr_projection = np.sum(raw_data[:, lr_indices, :], axis=1) ap_projection = np.sum(raw_data[:, ap_indices, :], axis=1) If using Cartesian indices, you can specify which slices or indices correspond to the axis of interest. Step 3: Visualize or Analyze the Projection The resulting 1D projection will represent signal variations over time along the chosen axis. For motion detection: Look for changes or peaks in the projection signal. Use thresholds or derivatives to classify motion events. Step 4: Validate Against the SI-Projection Compare the alternative projection with the SI-projection to ensure it aligns with the expected motion. 3. Application Examples Example 1: Detecting Horizontal Saccades Aggregate signal along the LR-axis to compute the horizontal projection. Analyze the projection for abrupt changes indicating rapid eye shifts (saccades). Example 2: Custom Projection for Diagonal Motion If the motion does not align with standard anatomical axes, project the data along an oblique axis. Define the oblique axis as a linear combination of standard axes: oblique_projection = np.sum(raw_data[:, :, :] * weight_matrix, axis=1) Here, weight_matrix defines the contribution of each axis to the oblique direction. 4. Considerations for Projections Signal-to-Noise Ratio (SNR) The effectiveness of the projection depends on the SNR of the data along the chosen axis. Ensure the data is preprocessed to remove noise that could obscure motion-related signals. Sensitivity to Artifacts Motion unrelated to the chosen axis (e.g., respiration or scanner vibrations) might still appear in the projection. Combining multiple projections can help differentiate these artifacts. Validation with Ground Truth If you are analyzing eye motion, consider validating the projections with eye-tracking data or known motion patterns. 5. Practical Code Example Here\u2019s an example for horizontal (LR) projection : import numpy as np import matplotlib.pyplot as plt # Simulated k-space data (replace with actual data) # Shape: (time_points, x_dim, y_dim, z_dim) raw_data = np.random.randn(100, 128, 128, 128) # Aggregate along LR-axis (x-axis) lr_projection = np.sum(raw_data[:, :, :, :], axis=2) # Adjust axis to your data structure # Plot the resulting projection signal over time plt.plot(np.mean(lr_projection, axis=(1, 2))) # Time-series of LR-projection signal plt.title('Left-Right Projection Signal') plt.xlabel('Time') plt.ylabel('Signal Intensity') plt.show() Conclusion You can compute projections along any desired axis (e.g., LR, AP) to align with the motion of interest. This flexibility is helpful for studying motion patterns that do not align with the SI-axis. Combining projections from multiple axes can improve the robustness of motion detection by capturing more dimensions of motion.","title":"SI projection and FID"},{"location":"nav/random_thoughts/#what-is-the-difference-between-si-projection-and-fid","text":"The SI-projection and FID (Free Induction Decay) are two different concepts in Magnetic Resonance Imaging (MRI) and spectroscopy, though they are related in the context of data acquisition and signal interpretation:","title":"\ud83c\udf38 What is the difference between SI-Projection and FID?"},{"location":"nav/random_thoughts/#1-free-induction-decay-fid","text":"Definition : FID is the raw signal detected immediately after the RF excitation pulse in MRI or spectroscopy. It represents the decaying sinusoidal signal generated by precessing magnetic moments of nuclei as they return to equilibrium. Characteristics : It is the time-domain signal acquired directly by the receiver coil. Contains all frequency components of the excited spins. The decay is due to T2* relaxation , which encompasses both dephasing caused by spin-spin interactions and magnetic field inhomogeneities. Usage : In spectroscopy, FID is Fourier transformed to extract the frequency-domain spectrum. In imaging, FID can be sampled to generate k-space data, which is later transformed into an image.","title":"1. Free Induction Decay (FID)"},{"location":"nav/random_thoughts/#2-si-projection-superior-inferior-projection","text":"Definition : SI-projection refers to a 1D or reduced-dimensionality representation of the MRI signal along the superior-inferior (head-to-foot) anatomical direction. It can be considered as a summation or integration of the MRI signal over the other spatial dimensions. Characteristics : It's a spatially resolved signal that contains positional information along the superior-inferior axis. Typically used in projection-based imaging methods or when simplifying multi-dimensional data. May be used for quick anatomical localization or as an intermediate step in reconstruction workflows. Usage : In motion-resolved imaging, SI-projections are often used to monitor respiratory or cardiac motion. For example, signals binned along the SI-axis can capture diaphragm movement during breathing.","title":"2. SI-Projection (Superior-Inferior Projection)"},{"location":"nav/random_thoughts/#key-differences","text":"Aspect FID SI-Projection Nature Time-domain raw signal Spatial projection signal Dimension 1D (time) 1D (spatial) Information Type Frequency and phase Spatial (position-based) Purpose Basis for Fourier transforms to generate spectra or k-space Basis for motion tracking or projection imaging Dependence Purely signal decay (T2*) Aggregates signal spatially","title":"Key Differences"},{"location":"nav/random_thoughts/#example-in-practice","text":"FID : Directly acquired after the RF pulse. Used in spectroscopy or as input for advanced reconstruction techniques in imaging. SI-Projection : Derived from imaging data along the superior-inferior axis. Often used in motion detection, like tracking respiratory motion during MRI.","title":"Example in Practice"},{"location":"nav/random_thoughts/#ok-then-for-the-raw-data-readouts-from-3d-radial-trajectory-how-can-i-extract-them-respectively","text":"Extracting FID and SI-projection from the raw data acquired using a 3D radial trajectory requires understanding the data's structure and processing steps. Here's how you can approach it:","title":"\ud83c\udf38 OK, then for the raw data readouts from 3D radial trajectory, how can I extract them respectively?"},{"location":"nav/random_thoughts/#1-understanding-the-raw-data","text":"3D Radial Trajectory : In a 3D radial acquisition, k-space is sampled along radial spokes that span the 3D volume. Each readout corresponds to a radial trajectory in k-space. Data typically includes both real and imaginary parts of the signal for each sampled point. Key Concepts : FID : The signal along the time dimension for a single trajectory before gridding or Fourier Transform. SI-Projection : Data integrated or projected along the superior-inferior axis after some spatial processing.","title":"1. Understanding the Raw Data"},{"location":"nav/random_thoughts/#2-extracting-fid","text":"The FID is essentially the raw time-domain signal acquired during each readout, and it can be extracted as follows: Identify the Readout Dimension : Locate the time-domain dimension in your raw data array (e.g., [readout, spokes, coils] ). Each trajectory in k-space corresponds to an FID signal. How to Extract FID: The FID is the very first sample of the raw readout for each spoke and each coil: fid = raw_data[0, :, coil_index] - `0`: Index for the first point in the readout (central k-space). - `:`: All spokes. - `coil_index`: A specific receive coil.","title":"2. Extracting FID"},{"location":"nav/random_thoughts/#3-extracting-si-projection","text":"The SI-projection requires spatial information, so some level of reconstruction or processing is needed: Reconstruct SI Direction : Use the radial trajectory information to identify k-space points corresponding to the superior-inferior axis. If trajectory data is stored in kx , ky , kz arrays, filter points for the SI-axis (e.g., where kx \u2248 ky \u2248 0 ). Sum or Integrate Data : Aggregate the raw data over the orthogonal dimensions: si_projection = raw_data.sum(axis=orthogonal_axes) - For a specific coil: si_projection = np.sum(raw_data[:, si_indices, coil_index], axis=1) Motion Binning (Optional) : If used for motion analysis, apply respiratory or cardiac binning based on temporal information or physiological signals.","title":"3. Extracting SI-Projection"},{"location":"nav/random_thoughts/#4-practical-notes","text":"Trajectory Information : Use the trajectory file (often provided by the scanner) to map k-space points to spatial coordinates. FFT for Image Space : If you need to convert raw k-space data to image space, perform an FFT on the summed or integrated k-space data along the appropriate dimensions. Coordinate Transformations : Ensure proper scaling and centering of k-space before projection or reconstruction.","title":"4. Practical Notes"},{"location":"nav/random_thoughts/#the-suitability-of-their-applications-and-motion-characteristics","text":"Both FID and SI-projection are valuable tools for detecting motion in MRI, but they differ significantly in how they are generated and what types of motion they can capture. Their suitability depends on the specific application and motion characteristics.","title":"The suitability of their applications and motion characteristics"},{"location":"nav/random_thoughts/#1-fid-free-induction-decay","text":"","title":"1. FID (Free Induction Decay)"},{"location":"nav/random_thoughts/#key-features","text":"What It Captures : The FID is sensitive to bulk magnetization changes , such as those caused by large-scale motion, scanner instability, or physiological processes that affect the entire signal simultaneously. Captures variations over time at the center of k-space, reflecting the overall signal behavior. How It Works for Motion Detection : Motion affects the uniformity of the magnetic field, causing variations in the FID amplitude or phase. Large-scale motion (e.g., head movement) can disrupt the signal decay pattern. Strengths : High sensitivity to gross motion : Since it captures the central k-space signal, even small deviations in motion can be detected as phase or amplitude variations. No spatial information required : FID does not depend on spatial encoding, making it computationally lightweight and simple to analyze. Ideal for detecting global motion patterns , such as large head movements. Limitations : Lacks spatial specificity: Cannot localize motion along specific axes or regions. Less effective for subtle, localized motion or small physiological movements (e.g., breathing, eye motion).","title":"Key Features:"},{"location":"nav/random_thoughts/#2-si-projection-superior-inferior-projection_1","text":"","title":"2. SI-Projection (Superior-Inferior Projection)"},{"location":"nav/random_thoughts/#key-features_1","text":"What It Captures : SI-projection aggregates data along the superior-inferior axis of the body, providing a motion-sensitive signal that captures shifts in this specific spatial direction. Reflects changes in signal distribution along the axis, making it more spatially specific than FID. How It Works for Motion Detection : Tracks changes in the superior-inferior axis signal due to respiratory motion , cardiac motion , or localized shifts (e.g., diaphragm or organ movement). Useful in motion-binned reconstruction methods to sort data into motion states. Strengths : Direction-specific : Captures motion primarily along the SI axis, which is beneficial for tracking respiratory motion or vertical head movements. Allows motion binning : By grouping data based on SI-projection values, you can segment the data into different motion states for retrospective correction. More sensitive to subtle, periodic physiological motion (e.g., respiration or heartbeat). Limitations : Less sensitive to non-SI motion: Does not effectively capture lateral (left-right) or anterior-posterior movements. Requires spatial trajectory information, increasing processing complexity compared to FID.","title":"Key Features:"},{"location":"nav/random_thoughts/#comparison-table","text":"Feature FID SI-Projection Captures Global magnetization changes Localized motion along SI axis Spatial Specificity None (global signal) High (specific to SI-axis motion) Motion Sensitivity Large-scale, global motion Subtle, periodic motion (e.g., respiration) Computation Lightweight, no trajectory needed Requires trajectory, spatial processing Best For Detecting global motion or scanner instabilities Tracking respiration or periodic SI motion","title":"Comparison Table"},{"location":"nav/random_thoughts/#3-which-to-use-when","text":"Use FID When : The primary concern is gross motion , such as patient movement or scanner instabilities. Simplicity is crucial (no need for spatial information or projections). You are monitoring the signal for artifacts in real-time (e.g., during acquisition). Use SI-Projection When : The focus is on physiological motion such as respiration or cardiac motion. You need motion-resolved reconstructions , where SI-projections enable binning into different motion states. Spatial specificity along the superior-inferior axis is necessary (e.g., for organ tracking or respiratory gating).","title":"3. Which to Use When?"},{"location":"nav/random_thoughts/#practical-applications","text":"Application Preferred Method Reason Head motion detection FID Captures global signal disturbances due to head shifts. Respiratory gating SI-Projection Sensitive to motion along the SI axis (e.g., diaphragm movement). Real-time motion artifact monitoring FID Simple, computationally efficient, no spatial info needed. Cardiac motion tracking SI-Projection Captures subtle periodic motion along the SI axis. Motion binning SI-Projection Enables grouping of data into distinct motion states.","title":"Practical Applications"},{"location":"nav/random_thoughts/#4-combining-both","text":"In some cases, FID and SI-projection can be used together : Use FID to monitor large-scale, abrupt motion (e.g., head movement). Use SI-projection for subtle, repetitive motion like respiration or cardiac cycles. By combining these approaches, you can achieve robust motion detection across a wide range of conditions. Let me know if you'd like help implementing this in your dataset!","title":"4. Combining Both"},{"location":"nav/random_thoughts/#how-can-si-projection-be-used-in-eye-motion-detection","text":"Using SI-projection to monitor eye motion can be effective, but its applicability depends on the specific characteristics of the eye motion being studied and how well the motion aligns with the superior-inferior (SI) axis. Here's a breakdown:","title":"\ud83c\udf38 How can SI-projection be used in eye motion detection?"},{"location":"nav/random_thoughts/#1-eye-motion-characteristics-and-si-projections","text":"Types of Eye Motion : Fixations : Minimal motion as the gaze focuses on a single point. Saccades : Rapid shifts in gaze between fixation points, often multidirectional. Microsaccades : Small, involuntary eye movements during fixation. Smooth Pursuit : Continuous tracking of a moving target. Motion Along the SI-Axis : If the primary eye motion occurs along the SI-axis (e.g., vertical saccades), the SI-projection would be sensitive to these changes. For horizontal (left-right) or oblique movements, the SI-projection would capture only a fraction of the overall motion, limiting its utility.","title":"1. Eye Motion Characteristics and SI-Projections"},{"location":"nav/random_thoughts/#2-advantages-of-using-si-projection-for-eye-motion","text":"Sensitivity to Vertical Motion : Eye movements with a strong vertical component (e.g., vertical saccades, gaze shifts, or blinking) will produce clear variations in the SI-projection. Can capture eyelid motion during blinks, as this involves a significant SI-axis component. Low Computational Complexity : SI-projections aggregate signal along the vertical axis, reducing the dimensionality of data and simplifying analysis. Applications in Eye Motion Studies : Blink Detection : Blinks primarily involve vertical motion of the eyelids, making SI-projection an effective tool. Vertical Gaze Tracking : Changes in vertical eye position can be directly observed in the projection signal. Motion Binning : SI-projections can sort data into states of upward, downward, or stationary gaze for further analysis.","title":"2. Advantages of Using SI-Projection for Eye Motion"},{"location":"nav/random_thoughts/#3-challenges-and-limitations","text":"Reduced Sensitivity to Horizontal or Oblique Motion : Eye motion along other axes (e.g., left-right saccades) may not produce significant variations in the SI-projection. Combining SI-projection with other projections (e.g., along the left-right or anterior-posterior axis) may be necessary for comprehensive motion tracking. Small Amplitude of Eye Movements : Eye movements, especially microsaccades or smooth pursuit, generate smaller displacements compared to gross motion (e.g., respiration). This requires high sensitivity in the SI-projection data. Artifact Sensitivity : Noise or signal disturbances unrelated to eye motion (e.g., system vibrations) might interfere with SI-projection signals.","title":"3. Challenges and Limitations"},{"location":"nav/random_thoughts/#4-practical-considerations","text":"To use SI-projection for eye motion detection , you might follow these steps:","title":"4. Practical Considerations"},{"location":"nav/random_thoughts/#step-1-preprocessing","text":"Data Source : Use raw k-space data or reconstructed signals, ensuring the trajectory is well-sampled. Trajectory Alignment : Verify that the SI-axis in the trajectory corresponds to the physical superior-inferior direction of the eye.","title":"Step 1: Preprocessing"},{"location":"nav/random_thoughts/#step-2-extract-si-projection","text":"Aggregate k-space data along the SI-axis: si_projection = np.sum(raw_data[:, si_indices, :], axis=1)","title":"Step 2: Extract SI-Projection"},{"location":"nav/random_thoughts/#step-3-analyze-motion","text":"Look for abrupt changes in SI-projection signals to detect vertical saccades or blinks: diff_signal = np.diff(si_projection, axis=0) Use thresholds to classify motion events (e.g., saccades, blinks, or fixations).","title":"Step 3: Analyze Motion"},{"location":"nav/random_thoughts/#step-4-validate-against-ground-truth","text":"Compare SI-projection results with independent eye-tracking data (e.g., from an EyeLink system) to ensure accuracy.","title":"Step 4: Validate Against Ground Truth"},{"location":"nav/random_thoughts/#5-example-application","text":"Here\u2019s how SI-projection could be applied for a blink detection task:","title":"5. Example Application"},{"location":"nav/random_thoughts/#case-blink-detection","text":"What Happens During a Blink? Eyelid movement occurs along the SI-axis, creating a sharp spike in the SI-projection signal. Signal Extraction : Calculate the SI-projection for each time point. Detect peaks or high-amplitude changes that correspond to eyelid motion. Results : The SI-projection provides a time-series representation of eye motion along the SI-axis. Peaks in the signal represent blink events.","title":"Case: Blink Detection"},{"location":"nav/random_thoughts/#comparison-to-other-techniques","text":"Feature SI-Projection for Eye Motion Dedicated Eye-Tracking Systems (e.g., EyeLink) Sensitivity High for vertical motion or blinks High for all types of eye motion Spatial Resolution Limited to the SI-axis Full 2D/3D tracking of eye position Ease of Use Low computational overhead Requires dedicated hardware and calibration Best Use Case Blinks, vertical gaze shifts Saccades, smooth pursuit, and detailed tracking","title":"Comparison to Other Techniques"},{"location":"nav/random_thoughts/#conclusion","text":"The SI-projection can be a simple, effective tool for detecting vertical eye motion , especially blinks and vertical gaze shifts. However, its application is limited to motion along the SI-axis, making it less effective for horizontal or complex multidirectional movements. For comprehensive eye motion analysis, pairing SI-projections with a dedicated eye-tracking system or additional k-space projections would provide better coverage. Let me know if you'd like help setting up such an analysis pipeline!","title":"Conclusion"},{"location":"nav/random_thoughts/#then-can-i-project-it-alternatively-to-another-direction-say-left-right-projection-instead-of-si-projection","text":"Yes, you can project the data along any direction (such as left-right, anterior-posterior, or any oblique direction) instead of the superior-inferior (SI) axis. This involves aggregating the k-space data along the desired direction or axis to extract a corresponding projection. Here's how you can do it step by step:","title":"\ud83c\udf38 Then can I project it alternatively to another direction, say left-right projection instead of SI-projection."},{"location":"nav/random_thoughts/#1-concept-of-alternative-projections","text":"Projection Direction : The SI-projection aggregates signal intensity along the superior-inferior axis. Similarly, you can compute a projection along: Left-Right (LR) : Captures horizontal motion or changes along the x-axis. Anterior-Posterior (AP) : Captures depth or forward-backward changes along the y-axis. Custom/Oblique Directions : Projects data onto a diagonal or other arbitrary axis. Utility : Choose the direction depending on the dominant motion in your data or the type of motion you're studying: LR projection is better for horizontal eye movements (e.g., horizontal saccades). AP projection may be relevant for depth changes or slight forward/backward tilting.","title":"1. Concept of Alternative Projections"},{"location":"nav/random_thoughts/#2-implementation-steps","text":"","title":"2. Implementation Steps"},{"location":"nav/random_thoughts/#step-1-identify-the-desired-axis","text":"Identify the direction in k-space that corresponds to the anatomical axis of interest. Example : SI-axis = Vertical direction = z z LR-axis = Horizontal direction = x x AP-axis = Depth direction = y y","title":"Step 1: Identify the Desired Axis"},{"location":"nav/random_thoughts/#step-2-aggregate-along-the-selected-axis","text":"Aggregate the k-space data along the desired axis by summing or integrating the data. # Example with numpy arrays lr_projection = np.sum(raw_data[:, lr_indices, :], axis=1) ap_projection = np.sum(raw_data[:, ap_indices, :], axis=1) If using Cartesian indices, you can specify which slices or indices correspond to the axis of interest.","title":"Step 2: Aggregate Along the Selected Axis"},{"location":"nav/random_thoughts/#step-3-visualize-or-analyze-the-projection","text":"The resulting 1D projection will represent signal variations over time along the chosen axis. For motion detection: Look for changes or peaks in the projection signal. Use thresholds or derivatives to classify motion events.","title":"Step 3: Visualize or Analyze the Projection"},{"location":"nav/random_thoughts/#step-4-validate-against-the-si-projection","text":"Compare the alternative projection with the SI-projection to ensure it aligns with the expected motion.","title":"Step 4: Validate Against the SI-Projection"},{"location":"nav/random_thoughts/#3-application-examples","text":"","title":"3. Application Examples"},{"location":"nav/random_thoughts/#example-1-detecting-horizontal-saccades","text":"Aggregate signal along the LR-axis to compute the horizontal projection. Analyze the projection for abrupt changes indicating rapid eye shifts (saccades).","title":"Example 1: Detecting Horizontal Saccades"},{"location":"nav/random_thoughts/#example-2-custom-projection-for-diagonal-motion","text":"If the motion does not align with standard anatomical axes, project the data along an oblique axis. Define the oblique axis as a linear combination of standard axes: oblique_projection = np.sum(raw_data[:, :, :] * weight_matrix, axis=1) Here, weight_matrix defines the contribution of each axis to the oblique direction.","title":"Example 2: Custom Projection for Diagonal Motion"},{"location":"nav/random_thoughts/#4-considerations-for-projections","text":"","title":"4. Considerations for Projections"},{"location":"nav/random_thoughts/#signal-to-noise-ratio-snr","text":"The effectiveness of the projection depends on the SNR of the data along the chosen axis. Ensure the data is preprocessed to remove noise that could obscure motion-related signals.","title":"Signal-to-Noise Ratio (SNR)"},{"location":"nav/random_thoughts/#sensitivity-to-artifacts","text":"Motion unrelated to the chosen axis (e.g., respiration or scanner vibrations) might still appear in the projection. Combining multiple projections can help differentiate these artifacts.","title":"Sensitivity to Artifacts"},{"location":"nav/random_thoughts/#validation-with-ground-truth","text":"If you are analyzing eye motion, consider validating the projections with eye-tracking data or known motion patterns.","title":"Validation with Ground Truth"},{"location":"nav/random_thoughts/#5-practical-code-example","text":"Here\u2019s an example for horizontal (LR) projection : import numpy as np import matplotlib.pyplot as plt # Simulated k-space data (replace with actual data) # Shape: (time_points, x_dim, y_dim, z_dim) raw_data = np.random.randn(100, 128, 128, 128) # Aggregate along LR-axis (x-axis) lr_projection = np.sum(raw_data[:, :, :, :], axis=2) # Adjust axis to your data structure # Plot the resulting projection signal over time plt.plot(np.mean(lr_projection, axis=(1, 2))) # Time-series of LR-projection signal plt.title('Left-Right Projection Signal') plt.xlabel('Time') plt.ylabel('Signal Intensity') plt.show()","title":"5. Practical Code Example"},{"location":"nav/random_thoughts/#conclusion_1","text":"You can compute projections along any desired axis (e.g., LR, AP) to align with the motion of interest. This flexibility is helpful for studying motion patterns that do not align with the SI-axis. Combining projections from multiple axes can improve the robustness of motion detection by capturing more dimensions of motion.","title":"Conclusion"},{"location":"nav/yiwei_cv/","text":"Yiwei Jia Ciao, I\u2019m Yiwei Jia. I received my master's degree in Electrical Engineering at RWTH Aachen University, Germany, and my bachelor's at Sun Yat-sen University, China. My master's thesis focuses on medical image processing and deep learning, especially image segmentation. During the internship practice, I also gained valuable experience in EEG processing and network architecture search. I joined MatTech Lab as a PhD student under the supervision of Prof. Benedetta Franceschiello, and worked as an affiliated PhD student with Prof. Jessica Bastiaansen at QIS Lab, University of Bern. My research is centered on developing an innovative anatomical and functional MR-Eye protocol capable of mitigating distortions caused by eye movements and addressing the trade-off between spatial and temporal resolution. This protocol will enable the acquisition of high-resolution (in micrometers) images with robustness to eye motion at the millisecond level. My objective is to implement algorithms that enhance image quality and facilitate the disentanglement of sub-millimeter eye structures on both clinical (3T) and research (7T) MRI scanners. You are welcome to reach out to me at LinkedIn !","title":"About me"},{"location":"nav/development/development-guide/","text":"Last updated January 28, 2024 How to test the local package? Virtual environment Create your virtual environment. python -m venv venv Environment activation Activate the virtual environment. source venv/bin/activate Install the requirements Install the necessary requirements to be able to test the application. pip install -r requirements.txt --no-cache-dir Development Make your changes as desired in the ./mkdocs_simple_blog folder. You can enjoy and change whatever you want, please have fun. ls mkdocs_simple_blog Build and Install Run the script that creates and installs the local package. python scripts/install_local.py Test Run the following command to run the server. mkdocs serve View Template Now you can access the http://127.0.0.1:8000/ Commit Style \u2699\ufe0f FEATURE \ud83d\udcdd PEP8 \ud83d\udccc ISSUE \ud83e\udeb2 BUG \ud83d\udcd8 DOCS \ud83d\udce6 PyPI \u2764\ufe0f\ufe0f TEST \u2b06\ufe0f CI/CD \u26a0\ufe0f SECURITY License This project is licensed under the terms of the MIT license.","title":"Development guide"},{"location":"nav/development/development-guide/#how-to-test-the-local-package","text":"","title":"How to test the local package?"},{"location":"nav/development/development-guide/#virtual-environment","text":"Create your virtual environment. python -m venv venv","title":"Virtual environment"},{"location":"nav/development/development-guide/#environment-activation","text":"Activate the virtual environment. source venv/bin/activate","title":"Environment activation"},{"location":"nav/development/development-guide/#install-the-requirements","text":"Install the necessary requirements to be able to test the application. pip install -r requirements.txt --no-cache-dir","title":"Install the requirements"},{"location":"nav/development/development-guide/#development","text":"Make your changes as desired in the ./mkdocs_simple_blog folder. You can enjoy and change whatever you want, please have fun. ls mkdocs_simple_blog","title":"Development"},{"location":"nav/development/development-guide/#build-and-install","text":"Run the script that creates and installs the local package. python scripts/install_local.py","title":"Build and Install"},{"location":"nav/development/development-guide/#test","text":"Run the following command to run the server. mkdocs serve","title":"Test"},{"location":"nav/development/development-guide/#view-template","text":"Now you can access the http://127.0.0.1:8000/","title":"View Template"},{"location":"nav/development/development-guide/#commit-style","text":"\u2699\ufe0f FEATURE \ud83d\udcdd PEP8 \ud83d\udccc ISSUE \ud83e\udeb2 BUG \ud83d\udcd8 DOCS \ud83d\udce6 PyPI \u2764\ufe0f\ufe0f TEST \u2b06\ufe0f CI/CD \u26a0\ufe0f SECURITY","title":"Commit Style"},{"location":"nav/development/development-guide/#license","text":"This project is licensed under the terms of the MIT license.","title":"License"},{"location":"nav/development/eye_atlases/","text":"\ud83d\udc3e Usage of eye atlases \ud83d\udcabMy question I want to use that for segment specific area for calculating SNR, CNR... Now I have template.nii.gz: atlas of the eye images (per sex), and max_prob_map.npy and max_prob_map.nii.gz: maximum probability maps (per sex), I also have my 3d image in pkl format. How can I do the registration and mask out the region I need? \ud83d\udca5Answer from ChatGPT To use the eye atlas and probability maps for segmenting specific regions and calculating metrics like SNR and CNR , you'll need to go through several steps, which include image registration, applying masks, and performing the desired calculations. Here\u2019s a general outline of the process: 1. Prepare the Data You have the following: template.nii.gz : A template atlas (likely a brain or eye atlas). max_prob_map.npy or max_prob_map.nii.gz : Maximum probability maps for each sex. 3D image in PKL format : This is your raw 3D image data. Objective : Register the 3D image to the template atlas , use the probability maps to mask the regions of interest, and calculate metrics like SNR (Signal-to-Noise Ratio) and CNR (Contrast-to-Noise Ratio) on those regions. 2. Convert PKL to NIfTI (if needed) If your 3D image is stored in a PKL file (Pickle format), you'll first need to load the PKL file and convert it to a NIfTI file (.nii.gz), which is a more common format for medical imaging tasks. Here's how you can do that in Python: import pickle import nibabel as nib import numpy as np # Load your PKL file with open('your_image.pkl', 'rb') as f: image_data = pickle.load(f) # Convert the image data to a NIfTI format image_nifti = nib.Nifti1Image(image_data, affine=np.eye(4)) # Save the NIfTI file nib.save(image_nifti, 'your_image.nii.gz') This will give you a NIfTI file of your image. 3. Register the 3D Image to the Template Atlas To align your 3D image with the template.nii.gz (the atlas), you can use image registration . A popular tool for this is FSL ( flirt or fnirt ) or ANTs (Advanced Normalization Tools). Here\u2019s how you can use FSL to register the images: FSL Registration Example: # Using FSL's flirt for linear registration flirt -in your_image.nii.gz -ref template.nii.gz -out registered_image.nii.gz -omat transform.mat This command will perform a linear registration of your image to the template and save the output as registered_image.nii.gz along with the transformation matrix transform.mat . If you need non-linear registration (e.g., to handle more complex deformations), you can use fnirt : # Using FSL's fnirt for non-linear registration fnirt --in=your_image.nii.gz --ref=template.nii.gz --out=registered_image_nl.nii.gz --aff=transform.mat After registration, your 3D image will be aligned to the template atlas . 4. Masking the Desired Region Using Probability Maps You can use the maximum probability map ( max_prob_map.nii.gz ) to identify and mask specific regions of interest in the registered image. The probability map indicates the likelihood of a certain structure or region in the atlas. To extract and mask specific regions based on the probability map: Using nibabel and numpy : import nibabel as nib import numpy as np # Load the registered 3D image and the probability map registered_image = nib.load('registered_image.nii.gz') prob_map = nib.load('max_prob_map.nii.gz') # Get the image data (as numpy arrays) image_data = registered_image.get_fdata() prob_map_data = prob_map.get_fdata() # Define a threshold (e.g., 50% probability) for masking threshold = 0.5 mask = prob_map_data > threshold # Apply the mask to the registered image masked_image = image_data * mask # Save the masked image masked_image_nifti = nib.Nifti1Image(masked_image, registered_image.affine) nib.save(masked_image_nifti, 'masked_image.nii.gz') This will create a masked image ( masked_image.nii.gz ) where the regions of interest, as identified by the probability map, are kept and everything else is set to zero. 5. Compute SNR and CNR After masking the desired regions, you can now compute SNR and CNR using the masked regions. The formulas for these metrics are: SNR : \\text{SNR} = \\frac{\\mu_{\\text{signal}}}{\\sigma_{\\text{noise}}} \\text{SNR} = \\frac{\\mu_{\\text{signal}}}{\\sigma_{\\text{noise}}} \\mu_{\\text{signal}} \\mu_{\\text{signal}} is the mean signal intensity in the region of interest. \\sigma_{\\text{noise}} \\sigma_{\\text{noise}} is the standard deviation of the signal in the background or noise region. CNR : \\text{CNR} = \\frac{\\mu_{\\text{signal}} - \\mu_{\\text{background}}}{\\sigma_{\\text{background}}} \\text{CNR} = \\frac{\\mu_{\\text{signal}} - \\mu_{\\text{background}}}{\\sigma_{\\text{background}}} \\mu_{\\text{signal}} \\mu_{\\text{signal}} is the mean intensity of the region of interest. \\mu_{\\text{background}} \\mu_{\\text{background}} is the mean intensity of the background. \\sigma_{\\text{background}} \\sigma_{\\text{background}} is the standard deviation of the background. Example Calculation in Python: # Define the background region (e.g., outside the mask) background_mask = ~mask # Calculate SNR for the masked region signal_region = image_data[mask] noise_region = image_data[background_mask] mean_signal = np.mean(signal_region) std_noise = np.std(noise_region) SNR = mean_signal / std_noise print(f\"SNR: {SNR}\") # Calculate CNR (assuming background region is well-defined) mean_background = np.mean(noise_region) std_background = np.std(noise_region) CNR = (mean_signal - mean_background) / std_background print(f\"CNR: {CNR}\") 6. Summary of Steps : Convert your PKL file to NIfTI format . Perform image registration to align your 3D image with the template atlas . Use the probability maps to create a mask of the region of interest. Apply the mask to the registered image to isolate the region of interest. Calculate SNR and CNR using the masked regions and background regions. Additional Notes: Thresholding the Probability Map : You can adjust the threshold value based on the specificity of the regions you want to mask. Post-processing : You may want to smooth the images or apply other filters depending on your analysis.","title":"Use atlases"},{"location":"nav/development/eye_atlases/#my-question","text":"I want to use that for segment specific area for calculating SNR, CNR... Now I have template.nii.gz: atlas of the eye images (per sex), and max_prob_map.npy and max_prob_map.nii.gz: maximum probability maps (per sex), I also have my 3d image in pkl format. How can I do the registration and mask out the region I need?","title":"\ud83d\udcabMy question"},{"location":"nav/development/eye_atlases/#answer-from-chatgpt","text":"To use the eye atlas and probability maps for segmenting specific regions and calculating metrics like SNR and CNR , you'll need to go through several steps, which include image registration, applying masks, and performing the desired calculations. Here\u2019s a general outline of the process:","title":"\ud83d\udca5Answer from ChatGPT"},{"location":"nav/development/eye_atlases/#1-prepare-the-data","text":"You have the following: template.nii.gz : A template atlas (likely a brain or eye atlas). max_prob_map.npy or max_prob_map.nii.gz : Maximum probability maps for each sex. 3D image in PKL format : This is your raw 3D image data. Objective : Register the 3D image to the template atlas , use the probability maps to mask the regions of interest, and calculate metrics like SNR (Signal-to-Noise Ratio) and CNR (Contrast-to-Noise Ratio) on those regions.","title":"1. Prepare the Data"},{"location":"nav/development/eye_atlases/#2-convert-pkl-to-nifti-if-needed","text":"If your 3D image is stored in a PKL file (Pickle format), you'll first need to load the PKL file and convert it to a NIfTI file (.nii.gz), which is a more common format for medical imaging tasks. Here's how you can do that in Python: import pickle import nibabel as nib import numpy as np # Load your PKL file with open('your_image.pkl', 'rb') as f: image_data = pickle.load(f) # Convert the image data to a NIfTI format image_nifti = nib.Nifti1Image(image_data, affine=np.eye(4)) # Save the NIfTI file nib.save(image_nifti, 'your_image.nii.gz') This will give you a NIfTI file of your image.","title":"2. Convert PKL to NIfTI (if needed)"},{"location":"nav/development/eye_atlases/#3-register-the-3d-image-to-the-template-atlas","text":"To align your 3D image with the template.nii.gz (the atlas), you can use image registration . A popular tool for this is FSL ( flirt or fnirt ) or ANTs (Advanced Normalization Tools). Here\u2019s how you can use FSL to register the images:","title":"3. Register the 3D Image to the Template Atlas"},{"location":"nav/development/eye_atlases/#fsl-registration-example","text":"# Using FSL's flirt for linear registration flirt -in your_image.nii.gz -ref template.nii.gz -out registered_image.nii.gz -omat transform.mat This command will perform a linear registration of your image to the template and save the output as registered_image.nii.gz along with the transformation matrix transform.mat . If you need non-linear registration (e.g., to handle more complex deformations), you can use fnirt : # Using FSL's fnirt for non-linear registration fnirt --in=your_image.nii.gz --ref=template.nii.gz --out=registered_image_nl.nii.gz --aff=transform.mat After registration, your 3D image will be aligned to the template atlas .","title":"FSL Registration Example:"},{"location":"nav/development/eye_atlases/#4-masking-the-desired-region-using-probability-maps","text":"You can use the maximum probability map ( max_prob_map.nii.gz ) to identify and mask specific regions of interest in the registered image. The probability map indicates the likelihood of a certain structure or region in the atlas. To extract and mask specific regions based on the probability map:","title":"4. Masking the Desired Region Using Probability Maps"},{"location":"nav/development/eye_atlases/#using-nibabel-and-numpy","text":"import nibabel as nib import numpy as np # Load the registered 3D image and the probability map registered_image = nib.load('registered_image.nii.gz') prob_map = nib.load('max_prob_map.nii.gz') # Get the image data (as numpy arrays) image_data = registered_image.get_fdata() prob_map_data = prob_map.get_fdata() # Define a threshold (e.g., 50% probability) for masking threshold = 0.5 mask = prob_map_data > threshold # Apply the mask to the registered image masked_image = image_data * mask # Save the masked image masked_image_nifti = nib.Nifti1Image(masked_image, registered_image.affine) nib.save(masked_image_nifti, 'masked_image.nii.gz') This will create a masked image ( masked_image.nii.gz ) where the regions of interest, as identified by the probability map, are kept and everything else is set to zero.","title":"Using nibabel and numpy:"},{"location":"nav/development/eye_atlases/#5-compute-snr-and-cnr","text":"After masking the desired regions, you can now compute SNR and CNR using the masked regions. The formulas for these metrics are: SNR : \\text{SNR} = \\frac{\\mu_{\\text{signal}}}{\\sigma_{\\text{noise}}} \\text{SNR} = \\frac{\\mu_{\\text{signal}}}{\\sigma_{\\text{noise}}} \\mu_{\\text{signal}} \\mu_{\\text{signal}} is the mean signal intensity in the region of interest. \\sigma_{\\text{noise}} \\sigma_{\\text{noise}} is the standard deviation of the signal in the background or noise region. CNR : \\text{CNR} = \\frac{\\mu_{\\text{signal}} - \\mu_{\\text{background}}}{\\sigma_{\\text{background}}} \\text{CNR} = \\frac{\\mu_{\\text{signal}} - \\mu_{\\text{background}}}{\\sigma_{\\text{background}}} \\mu_{\\text{signal}} \\mu_{\\text{signal}} is the mean intensity of the region of interest. \\mu_{\\text{background}} \\mu_{\\text{background}} is the mean intensity of the background. \\sigma_{\\text{background}} \\sigma_{\\text{background}} is the standard deviation of the background.","title":"5. Compute SNR and CNR"},{"location":"nav/development/eye_atlases/#example-calculation-in-python","text":"# Define the background region (e.g., outside the mask) background_mask = ~mask # Calculate SNR for the masked region signal_region = image_data[mask] noise_region = image_data[background_mask] mean_signal = np.mean(signal_region) std_noise = np.std(noise_region) SNR = mean_signal / std_noise print(f\"SNR: {SNR}\") # Calculate CNR (assuming background region is well-defined) mean_background = np.mean(noise_region) std_background = np.std(noise_region) CNR = (mean_signal - mean_background) / std_background print(f\"CNR: {CNR}\")","title":"Example Calculation in Python:"},{"location":"nav/development/eye_atlases/#6-summary-of-steps","text":"Convert your PKL file to NIfTI format . Perform image registration to align your 3D image with the template atlas . Use the probability maps to create a mask of the region of interest. Apply the mask to the registered image to isolate the region of interest. Calculate SNR and CNR using the masked regions and background regions.","title":"6. Summary of Steps:"},{"location":"nav/development/eye_atlases/#additional-notes","text":"Thresholding the Probability Map : You can adjust the threshold value based on the specificity of the regions you want to mask. Post-processing : You may want to smooth the images or apply other filters depending on your analysis.","title":"Additional Notes:"},{"location":"nav/development/release-notes/","text":"March 25, 2023 v0.0.9 \ud83d\udce6 PyPI - Build 0.0.9 \ud83d\udccc ISSUE-27 - Minify CSS and JS \ud83d\udccc ISSUE-26 - Code indentation error v0.0.8 \ud83d\udce6 PyPI - Build 0.0.8 \ud83d\udccc ISSUE-22 - Search v0.0.7 \ud83d\udce6 PyPI - Build 0.0.7 \ud83d\udccc ISSUE-21 - Fix the extra_javascript include in base.html v0.0.6 \ud83d\udce6 PyPI - Build 0.0.6 \ud83d\udccc ISSUE-16 - Include option to add and remove components v0.0.5 \ud83d\udce6 PyPI - Build 0.0.5 \ud83d\udccc ISSUE-13 - Inclusion option to preview next and previous post \ud83d\udccc ISSUE-7 - Add additional theme settings \ud83d\udccc ISSUE-5 - Adjust blog interface for mobile devices \ud83d\udccc ISSUE-2 - Create dark mode and other color options v0.0.4 \ud83d\udce6 PyPI - Build 0.0.4 \ud83d\udccc ISSUE-1 - Create custom menu v0.0.3 \ud83d\udce6 PyPI - Build 0.0.3 v0.0.2 \ud83d\udce6 PyPI - Build 0.0.2 v0.0.1 \ud83d\udce6 PyPI - Build 0.0.1","title":"Release notes"},{"location":"nav/development/release-notes/#v009","text":"\ud83d\udce6 PyPI - Build 0.0.9 \ud83d\udccc ISSUE-27 - Minify CSS and JS \ud83d\udccc ISSUE-26 - Code indentation error","title":"v0.0.9"},{"location":"nav/development/release-notes/#v008","text":"\ud83d\udce6 PyPI - Build 0.0.8 \ud83d\udccc ISSUE-22 - Search","title":"v0.0.8"},{"location":"nav/development/release-notes/#v007","text":"\ud83d\udce6 PyPI - Build 0.0.7 \ud83d\udccc ISSUE-21 - Fix the extra_javascript include in base.html","title":"v0.0.7"},{"location":"nav/development/release-notes/#v006","text":"\ud83d\udce6 PyPI - Build 0.0.6 \ud83d\udccc ISSUE-16 - Include option to add and remove components","title":"v0.0.6"},{"location":"nav/development/release-notes/#v005","text":"\ud83d\udce6 PyPI - Build 0.0.5 \ud83d\udccc ISSUE-13 - Inclusion option to preview next and previous post \ud83d\udccc ISSUE-7 - Add additional theme settings \ud83d\udccc ISSUE-5 - Adjust blog interface for mobile devices \ud83d\udccc ISSUE-2 - Create dark mode and other color options","title":"v0.0.5"},{"location":"nav/development/release-notes/#v004","text":"\ud83d\udce6 PyPI - Build 0.0.4 \ud83d\udccc ISSUE-1 - Create custom menu","title":"v0.0.4"},{"location":"nav/development/release-notes/#v003","text":"\ud83d\udce6 PyPI - Build 0.0.3","title":"v0.0.3"},{"location":"nav/development/release-notes/#v002","text":"\ud83d\udce6 PyPI - Build 0.0.2","title":"v0.0.2"},{"location":"nav/development/release-notes/#v001","text":"\ud83d\udce6 PyPI - Build 0.0.1","title":"v0.0.1"},{"location":"nav/papers/SISMIK_for_brain_MRI/","text":"SISMIK for brain MRI: Deep-learning-based motion estimation and model-based motion correction in k-space A retrospective method for motion estimation and correction to tackle the problem of in-plane rigid-body motion, apt for classical 2D Spin-Echo scans of the brain, which are regularly used in clinical practice. Note: TSE is in Cartesian space... If it is combined with the FID signal, it would also estimate the rotation and translation shift?","title":"SISMIK DL motion"},{"location":"nav/papers/SISMIK_for_brain_MRI/#sismik-for-brain-mri-deep-learning-based-motion-estimation-and-model-based-motion-correction-in-k-space","text":"A retrospective method for motion estimation and correction to tackle the problem of in-plane rigid-body motion, apt for classical 2D Spin-Echo scans of the brain, which are regularly used in clinical practice. Note: TSE is in Cartesian space... If it is combined with the FID signal, it would also estimate the rotation and translation shift?","title":"SISMIK for brain MRI: Deep-learning-based motion estimation and model-based motion correction in k-space"},{"location":"nav/papers/adaptive_bulk_motion/","text":"Paper link paper link Extraction from the paper Bulk motion in the presence of bulk patient motion (i.e. voluntary or involuntary patient movement resulting in translation, rotation or elastic deformations of the body). Gaps/difficulties: these techniques for respiration cannot correct for bulk motion which includes voluntary and involuntary patient movement resulting in translation, rotation or elastic deformation of the body.3 One possibility to remove bulk motion is to exclude corrupted data retrospectively. Because the duration and number of intervals with bulk motion is not known in advance, k\u2010space must be acquired with significant oversampling to provide sufficient k\u2010space coverage. This prolongs the scan time unnecessarily if no (or only short) intervals of bulk motion occur, or results in residual undersampling artifacts if more bulk motion than expected occurs. Limitations: reject data by bulk motion but does not compensate for respiratory motion -> need second processing step. Method Data\u2010consistency\u2010driven image stabilization technique that detects and excludes bulk movements during data acquisition. Data corrupted by bulk motion were successfully detected and excluded from image reconstruction An overall increase in image sharpness and reduction of streaking and shine\u2010through artifacts were seen in the volunteer study, as well as in the neck and knee scans. The proposed technique enables automatic real\u2010time detection and exclusion of bulk motion during MR examinations without user interaction. the technique automatically detects motion\u2010free data windows and stops the scan if a window with sufficient length has been identified. What sequence a T1\u2010weighted radial 3D gradient\u2010echo sequence with both RF and gradient spoiling. The stack\u2010of\u2010stars scheme with Cartesian encoding along the kz dimension and radial sampling in the kx\u2013ky plane is used for volumetric k\u2010space coverage. To obtain motion information, 32 samples of the FID signal are acquired after rewinding the slice selection gradient, which increases the TE by less than 0.5 ms. Yiwei: so that means each FID signal is acquired after one k_{x}-k_{y} k_{x}-k_{y} <span class=\"arithmatex\"><span class=\"MathJax_Preview\">k_{x}-k_{y}</span><script type=\"math/tex\">k_{x}-k_{y} plane excited. The FID approach is used in this work to derive a motion signal. It has been shown previously that variations in the signal intensity ratio between different receive coils allow the identification of bulk motion. There are two alternative self-navigation techs: non-phase-encoded k-space center lines->for stack-of-stars sequence, non-Cartesian 3D trajectories good -> for Cartesian 3D sampling it requires the acquisition of additional non-phase-encoded k-space lines, which prolongs the scan time. Free induction decay navigators: sample the center of k-space without spatial encoding immediately after the RF excitation -> selected 32 samples of FID signals -> image calculation environment(ICE) framework -> detection of bulk motion. The first and last four samples of each FID signal are discarded to account for pre\u2010 and post\u2010ringing. For ach projection, the summed values from the different coils are concatenated into a projection vector of length Ncoil. Motion is detected by calculating the correlation coefficient between a reference projection vector and all other projection vectors. A low correlation coefficient implies that the load distribution of the coil elements has changed. This indicates that the patient position has been altered between the projections. Calculate the reference projection vector, which is defined as the projection with highest overall correlation to all previously acquired projections, is dynamically updated after each projection. In the current implementation, the reference projection is therefore only calculated within a sliding window containing the previous 100 projections. Outlier projections, which indicate bulk motion, are identified using a user\u2010defined acceptance threshold for the correlation coefficient. Details during imaging a scan without bulk motion was performed as reference. max scan time: 5min11s -- 1500 projections the acceptance window was set to 400 projections (1min23s) -- fulfill Nyquist criterion for radial sampling (??) Information about FID signal FID signals ref link When an RF pulse is turned off, protons begin to lose phase coherence, resulting in an exponential decrease of the signal intensity to zero. This signal decay is called free induction decay (FID) (Currie et al., 2013). FID signal has a resonant frequency, which takes the form of a sine wave, which is at its highest immediately after the RF pulse is switched off and decreases as relaxation occurs. An RF pulse at any flip angle can create an FID because some protons in longitudinal magnetization are tipped into transverse magnetization. How to extract the FID from raw data 1. Understand the Acquisition Setup FID signals are collected before the imaging gradients are fully applied. The FID contains information about the global magnetization state of the tissue but has no spatial encoding yet. In radial stack-of-star sequences, the FID signal corresponds to data at or near the k-space center, typically sampled immediately after the RF pulse. why The FID signal is generated immediately after the RF excitation pulse, as soon as the spins start precessing. The gradients needed for spatial encoding are typically applied after the FID starts . This delay is necessary because: The RF pulse and gradients require separate timing to avoid interference. Applying gradients during the RF pulse or immediately afterward can distort the FID and reduce signal quality. The initial part of the signal , acquired before the gradients are active, represents the \"pure\" FID\u2014this is the part you extract for motion detection or other analyses. 2. Identify the FID Portion in Raw Data The FID is typically acquired as the initial part of each readout, before gradients start encoding spatial information. 3. Extract k-Space Center The FID signal corresponds to the low spatial frequency (k-space center) information. You can identify this by: Temporal Location: Extract data samples immediately after the RF pulse and before the imaging gradients are active. 4. FID in the Context of a Stack-of-Star Sequence In radial sequences like stack-of-stars, k-space is sampled using radial trajectories starting at the center. At the start of each radial spoke, the k-space center is sampled, and this corresponds to the initial FID signal . Because the gradients are weak or inactive near the k-space center, the early signal is dominated by the FID, with minimal spatial encoding effects. 5. How Motion Affects the FID Motion impacts the FID signal in the following ways: a. Phase Changes Due to Motion Motion alters the relative positions of spins in the magnetic field, which changes their precession phase. These phase changes are global (affecting the whole signal) and become apparent in the FID because the FID is the earliest signal captured after excitation. For example, a small head movement during the acquisition will introduce phase shifts, which can be detected in the FID signal. b. Amplitude Modulation Motion can cause signal dephasing, where spins lose coherence due to changing positions or velocities. This results in a reduced FID amplitude. Additionally, periodic motions (e.g., respiratory or cardiac motion) may modulate the FID signal amplitude in a predictable way. c. Frequency Shifts If the motion involves velocity (e.g., blood flow or respiratory motion), it can introduce frequency shifts due to the Doppler effect. These shifts can also be detected in the FID. Information about stack-of-star sequence Stack-of-stars Illustration But it does not start from the k-space center? Here is the described sequence: Data are acquired using a T1\u2010weighted radial 3D gradient\u2010echo sequence with both RF and gradient spoiling . The stack\u2010of\u2010stars scheme with Cartesian encoding along the kz dimension and radial sampling in the kx\u2013ky plane is used for volumetric k\u2010space coverage. After spectral fat suppression using a chemical shift selective (CHESS) pulse, all partitions for one radial angle are acquired with centric reordering before the readout scheme is repeated for subsequent radial projections. Golden\u2010angle ordering with an angular increment of 111.246\u00b025 is used for continuous data acquisition.","title":"bulk motion"},{"location":"nav/papers/adaptive_bulk_motion/#paper-link","text":"paper link","title":"Paper link"},{"location":"nav/papers/adaptive_bulk_motion/#extraction-from-the-paper","text":"Bulk motion in the presence of bulk patient motion (i.e. voluntary or involuntary patient movement resulting in translation, rotation or elastic deformations of the body). Gaps/difficulties: these techniques for respiration cannot correct for bulk motion which includes voluntary and involuntary patient movement resulting in translation, rotation or elastic deformation of the body.3 One possibility to remove bulk motion is to exclude corrupted data retrospectively. Because the duration and number of intervals with bulk motion is not known in advance, k\u2010space must be acquired with significant oversampling to provide sufficient k\u2010space coverage. This prolongs the scan time unnecessarily if no (or only short) intervals of bulk motion occur, or results in residual undersampling artifacts if more bulk motion than expected occurs. Limitations: reject data by bulk motion but does not compensate for respiratory motion -> need second processing step. Method Data\u2010consistency\u2010driven image stabilization technique that detects and excludes bulk movements during data acquisition. Data corrupted by bulk motion were successfully detected and excluded from image reconstruction An overall increase in image sharpness and reduction of streaking and shine\u2010through artifacts were seen in the volunteer study, as well as in the neck and knee scans. The proposed technique enables automatic real\u2010time detection and exclusion of bulk motion during MR examinations without user interaction. the technique automatically detects motion\u2010free data windows and stops the scan if a window with sufficient length has been identified. What sequence a T1\u2010weighted radial 3D gradient\u2010echo sequence with both RF and gradient spoiling. The stack\u2010of\u2010stars scheme with Cartesian encoding along the kz dimension and radial sampling in the kx\u2013ky plane is used for volumetric k\u2010space coverage. To obtain motion information, 32 samples of the FID signal are acquired after rewinding the slice selection gradient, which increases the TE by less than 0.5 ms. Yiwei: so that means each FID signal is acquired after one k_{x}-k_{y} k_{x}-k_{y} <span class=\"arithmatex\"><span class=\"MathJax_Preview\">k_{x}-k_{y}</span><script type=\"math/tex\">k_{x}-k_{y} plane excited. The FID approach is used in this work to derive a motion signal. It has been shown previously that variations in the signal intensity ratio between different receive coils allow the identification of bulk motion. There are two alternative self-navigation techs: non-phase-encoded k-space center lines->for stack-of-stars sequence, non-Cartesian 3D trajectories good -> for Cartesian 3D sampling it requires the acquisition of additional non-phase-encoded k-space lines, which prolongs the scan time. Free induction decay navigators: sample the center of k-space without spatial encoding immediately after the RF excitation -> selected 32 samples of FID signals -> image calculation environment(ICE) framework -> detection of bulk motion. The first and last four samples of each FID signal are discarded to account for pre\u2010 and post\u2010ringing. For ach projection, the summed values from the different coils are concatenated into a projection vector of length Ncoil. Motion is detected by calculating the correlation coefficient between a reference projection vector and all other projection vectors. A low correlation coefficient implies that the load distribution of the coil elements has changed. This indicates that the patient position has been altered between the projections. Calculate the reference projection vector, which is defined as the projection with highest overall correlation to all previously acquired projections, is dynamically updated after each projection. In the current implementation, the reference projection is therefore only calculated within a sliding window containing the previous 100 projections. Outlier projections, which indicate bulk motion, are identified using a user\u2010defined acceptance threshold for the correlation coefficient. Details during imaging a scan without bulk motion was performed as reference. max scan time: 5min11s -- 1500 projections the acceptance window was set to 400 projections (1min23s) -- fulfill Nyquist criterion for radial sampling (??)","title":"Extraction from the paper"},{"location":"nav/papers/adaptive_bulk_motion/#information-about-fid-signal","text":"","title":"Information about FID signal"},{"location":"nav/papers/adaptive_bulk_motion/#fid-signals","text":"ref link When an RF pulse is turned off, protons begin to lose phase coherence, resulting in an exponential decrease of the signal intensity to zero. This signal decay is called free induction decay (FID) (Currie et al., 2013). FID signal has a resonant frequency, which takes the form of a sine wave, which is at its highest immediately after the RF pulse is switched off and decreases as relaxation occurs. An RF pulse at any flip angle can create an FID because some protons in longitudinal magnetization are tipped into transverse magnetization.","title":"FID signals"},{"location":"nav/papers/adaptive_bulk_motion/#how-to-extract-the-fid-from-raw-data","text":"","title":"How to extract the FID from raw data"},{"location":"nav/papers/adaptive_bulk_motion/#1-understand-the-acquisition-setup","text":"FID signals are collected before the imaging gradients are fully applied. The FID contains information about the global magnetization state of the tissue but has no spatial encoding yet. In radial stack-of-star sequences, the FID signal corresponds to data at or near the k-space center, typically sampled immediately after the RF pulse. why The FID signal is generated immediately after the RF excitation pulse, as soon as the spins start precessing. The gradients needed for spatial encoding are typically applied after the FID starts . This delay is necessary because: The RF pulse and gradients require separate timing to avoid interference. Applying gradients during the RF pulse or immediately afterward can distort the FID and reduce signal quality. The initial part of the signal , acquired before the gradients are active, represents the \"pure\" FID\u2014this is the part you extract for motion detection or other analyses. 2. Identify the FID Portion in Raw Data The FID is typically acquired as the initial part of each readout, before gradients start encoding spatial information. 3. Extract k-Space Center The FID signal corresponds to the low spatial frequency (k-space center) information. You can identify this by: Temporal Location: Extract data samples immediately after the RF pulse and before the imaging gradients are active. 4. FID in the Context of a Stack-of-Star Sequence In radial sequences like stack-of-stars, k-space is sampled using radial trajectories starting at the center. At the start of each radial spoke, the k-space center is sampled, and this corresponds to the initial FID signal . Because the gradients are weak or inactive near the k-space center, the early signal is dominated by the FID, with minimal spatial encoding effects. 5. How Motion Affects the FID Motion impacts the FID signal in the following ways: a. Phase Changes Due to Motion Motion alters the relative positions of spins in the magnetic field, which changes their precession phase. These phase changes are global (affecting the whole signal) and become apparent in the FID because the FID is the earliest signal captured after excitation. For example, a small head movement during the acquisition will introduce phase shifts, which can be detected in the FID signal. b. Amplitude Modulation Motion can cause signal dephasing, where spins lose coherence due to changing positions or velocities. This results in a reduced FID amplitude. Additionally, periodic motions (e.g., respiratory or cardiac motion) may modulate the FID signal amplitude in a predictable way. c. Frequency Shifts If the motion involves velocity (e.g., blood flow or respiratory motion), it can introduce frequency shifts due to the Doppler effect. These shifts can also be detected in the FID.","title":"1. Understand the Acquisition Setup"},{"location":"nav/papers/adaptive_bulk_motion/#information-about-stack-of-star-sequence","text":"Stack-of-stars Illustration But it does not start from the k-space center? Here is the described sequence: Data are acquired using a T1\u2010weighted radial 3D gradient\u2010echo sequence with both RF and gradient spoiling . The stack\u2010of\u2010stars scheme with Cartesian encoding along the kz dimension and radial sampling in the kx\u2013ky plane is used for volumetric k\u2010space coverage. After spectral fat suppression using a chemical shift selective (CHESS) pulse, all partitions for one radial angle are acquired with centric reordering before the readout scheme is repeated for subsequent radial projections. Golden\u2010angle ordering with an angular increment of 111.246\u00b025 is used for continuous data acquisition.","title":"Information about stack-of-star sequence"},{"location":"nav/papers/bulk/","text":"","title":"Bulk"},{"location":"nav/papers/fetal/","text":"","title":"Fetal"},{"location":"nav/papers/retrospective_correction/","text":"Retrospective correction of head motion using measurements from an electromagnetic tracker Paper link What sequence? T1-weighted MPRAGE sequence What recon? Each k-space line from each coil -> corrected for phase ramps -> regridded in 3D k-space (NUFFT) with the estimated rigid body motion param as suggested by Gallichan [23] Motion-corrected images were reconed for each coil -> the adaptive combine method was used to create the final image. What Electromagnetic tracking data look like? Translation and rotation of each scan.. How do we get the reference image? Still images (include involuntary movements) What is the protocol? Phantom experiments: a pineapple -> 8 times(after movements) * 45s Volunteer experiments: 6 volunteers, each volunteer -> 4 volumes still -> used as no motion reference image four movements (random position) nodding up down continuous head nodding: up-down, down-up every 30 seconds (not explained too much) Patient scans: eight pediatric patients (Not explain too much) Metrics: normalized root-mean-square error (NRMSE) and the structural similarity (SSIM) index (using FSL tool BET [26]) relative to this motion-free reference image. a reference-free metric, average edge strength (AES) was calculated for each scan As suggested by Aksoy et al. [27], AES values were normalized by the corresponding slice in the \u201cno motion\u201d dataset. Limitation: only in-plane motion, but inability of correction for though-plane motion in 2D sequence.","title":"head motion"},{"location":"nav/secret_garden/SOP_energy/","text":"Start from January, 2025 for a balanced, resilient, powerful phd life.","title":"SOP for energy"},{"location":"nav/setup/colors/","text":"January 17, 2023 Changing the colors Color scheme Default: light theme: theme_style: light light dark Primary color Default: black theme: colors: primary: black black blue green yellow orange purple red white Text color Default: black theme: colors: text: black black blue green yellow orange purple red white Title color Default: black theme: colors: title: black black blue green yellow orange purple red white Background color Default: white theme: colors: background: white black blue green yellow orange purple red white","title":"Colors"},{"location":"nav/setup/colors/#changing-the-colors","text":"","title":"Changing the colors"},{"location":"nav/setup/colors/#color-scheme","text":"Default: light theme: theme_style: light light dark","title":"Color scheme"},{"location":"nav/setup/colors/#primary-color","text":"Default: black theme: colors: primary: black black blue green yellow orange purple red white","title":"Primary color"},{"location":"nav/setup/colors/#text-color","text":"Default: black theme: colors: text: black black blue green yellow orange purple red white","title":"Text color"},{"location":"nav/setup/colors/#title-color","text":"Default: black theme: colors: title: black black blue green yellow orange purple red white","title":"Title color"},{"location":"nav/setup/colors/#background-color","text":"Default: white theme: colors: background: white black blue green yellow orange purple red white","title":"Background color"},{"location":"nav/setup/components/","text":"January 21, 2023 Enable and Disable Components Site Name Default: true theme: components: site_name: true true false Title Default: true theme: components: title: true true false Menu Default: true theme: components: menu: true true false Preview Default: true theme: components: preview: true true false Footer Default: true theme: components: footer: true true false","title":"Components"},{"location":"nav/setup/components/#enable-and-disable-components","text":"","title":"Enable and Disable Components"},{"location":"nav/setup/components/#site-name","text":"Default: true theme: components: site_name: true true false","title":"Site Name"},{"location":"nav/setup/components/#title","text":"Default: true theme: components: title: true true false","title":"Title"},{"location":"nav/setup/components/#menu","text":"Default: true theme: components: menu: true true false","title":"Menu"},{"location":"nav/setup/components/#preview","text":"Default: true theme: components: preview: true true false","title":"Preview"},{"location":"nav/setup/components/#footer","text":"Default: true theme: components: footer: true true false","title":"Footer"},{"location":"nav/setup/favicon/","text":"January 17, 2023 Changing the favicon theme: favicon: assets/favicon.ico","title":"Favicon"},{"location":"nav/setup/favicon/#changing-the-favicon","text":"theme: favicon: assets/favicon.ico","title":"Changing the favicon"},{"location":"nav/setup/fonts/","text":"January 17, 2023 Changing the fonts Site name style Default: normal theme: site_name_style: normal normal bold italic scratched underline overline Title style Default: bold theme: title_style: bold normal bold italic scratched underline overline","title":"Fonts"},{"location":"nav/setup/fonts/#changing-the-fonts","text":"","title":"Changing the fonts"},{"location":"nav/setup/fonts/#site-name-style","text":"Default: normal theme: site_name_style: normal normal bold italic scratched underline overline","title":"Site name style"},{"location":"nav/setup/fonts/#title-style","text":"Default: bold theme: title_style: bold normal bold italic scratched underline overline","title":"Title style"},{"location":"nav/setup/logo/","text":"January 17, 2023 Changing the logo theme: logo: assets/logo.png","title":"Logo"},{"location":"nav/setup/logo/#changing-the-logo","text":"theme: logo: assets/logo.png","title":"Changing the logo"}]}